# Target-Entropy-Encoding
An enhanced nominal categorical data encoding algorithm for binary classification in machine learning 

Most machine learning classification problems involve the use of datasets containing a mixture of numeric and categorical feature. Encoding categorical data is an important step in feature engineering since most machine learning algorithms only accept numeric inputs. However, the most widely used encoding algorithms are memory inefficient. More memory-efficient encoding algorithms have been developed, but these algorithms tend to trade off performance for improved memory usage. This study is aimed at developing an enhanced nominal categorical data encoding algorithm for binary classification in machine learning.

The strengths of target encoding were combined with concepts from Shannon’s information theory to develop an enhanced nominal data encoding algorithm, in an attempt to improve on the memory inefficiency of existing encoders and classification accuracy of machine learning classifiers. The dataset used in this study is a record of Toyota car prices from 2011 to 2020, sourced from https://www.kaggle.com. A machine learning classification model was developed to predict the price class of cars as above or below average using six machine learning classifiers (Logistic regression, Gaussian Naïve Bayes, Support Vector Machine, Decision Tree, Random Forest and Neural Networks). The dataset was encoded using three existing encoding algorithms (One Hot Encoding, Feature Hashing and Weight of Evidence Encoding), and the developed encoding algorithm (Target Entropy Encoding). The encoded datasets were fed in turns to each machine learning classifier, the accuracies of the classifiers and the dimensionality of the encoded datasets were recorded. 

When only the encoded categorical features were used in classification, the average accuracies obtained were 65.53%, 72.49%, 74.05%, and 79.10% for Weight of Evidence Encoding, One Hot Encoding, Feature Hashing, and the developed algorithm respectively. When mixed features were used in classification, the average accuracies obtained were 78.93%, 80.29%, 82.15%, and 85.41% for One Hot Encoding, Weight of Evidence Encoding, Feature Hashing, and the developed algorithm respectively. One hot encoding increased the dimensionality of the dataset from 9 6 columns to 32 columns after encoding while feature hashing increase dimensionality from 9 to 33 columns after encoding. 

There was no increase in dimensionality after encoding with the developed algorithm and weight of evidence encoding, showing their superior memory efficiency over one hot encoding and feature hashing. The developed algorithm (Target Entropy Encoding) showed consistent performance across all the machine learning classifiers unlike the other encoders which showed significant drop in performance when used with certain classification algorithms. These qualities make the proposed algorithm superior to One Hot Encoding (and its modifications), Feature Hashing, and Weight of Evidence Encoding for binary classification tasks in machine learning.
